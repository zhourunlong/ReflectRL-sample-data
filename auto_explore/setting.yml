bf16: true
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
cache_dir: model/
ckpt_path: results/
critic_layer_type: linear
critic_update_freq: 5
critic_update_iter: 5
curriculum_index: 0
depth_curriculum: true
disable_alfworld: true
disable_dropout: false
discretize_actions: true
easy: false
entropy_coef: 0.0
env: auto_explore
eval_reps: 10
few_data: 0
first_step: false
fp16: false
gradient_accumulation_steps: 1
gradient_checkpointing: true
group_by_length: false
horizon: 15
is_slippery: false
learning_rate: 0.0002
leaveout_prob: 0.5
load_dir: results/517_supervised_pretrain/checkpoint-5000/
logging_steps: 10
lora_alpha: 16
lora_dropout: 0.1
lora_r: 64
lr_scheduler_type: constant
map_size: 4
max_grad_norm: 0.3
max_new_tokens: 1
max_seq_length: 1024
max_steps: 5000
merge_after_first_k: 3
merge_first_two: true
mode: train_RLFT
model_name: gpt2-xl
num_train_epochs: 1
optim: paged_adamw_32bit
per_device_eval_batch_size: 4
per_device_train_batch_size: 1
pickup_curriculum: false
ppo_clip_coef: 0.1
ppo_update_iter: 3
random_map: false
reflect: true
reflect_load_dir: results/auto_explore_reflectioner/checkpoint-15000/
reflect_model_name: gpt2-xl
reflect_prob: 1.0
replay_buffer_size: 50
repo_dir: data/auto_explore/repos_filtered/
sandbox_dir: /dev/shm/
save_steps: 100
save_total_limit: 50
shared_critic: false
shrink_head: true
shuffle_action: true
task_file: data/auto_explore/tasks_filtered/train.json
temperature: 1
top_k: 99999
top_p: 1
trainer: pg
use_4bit: false
use_8bit: true
use_critic: false
use_nested_quant: false
warmup_ratio: 0.03
weight_decay: 0.001
with_prompt: false
